library(arrow)
library(glue)
library(here)
library(httr)
library(janitor)
library(purrr)
library(readr)
library(tidyverse)
source("02_scripts/01_config.R")
library(here)
# Load credentials from R environment
usr <- Sys.getenv("TABMON_USR")
psswd <- Sys.getenv("TABMON_PSSWD")
# Stop with error if credentials are not set
if (usr == "" || psswd == "") {
stop("Missing authentication credentials! Please set TABMON_USR and TABMON_PSSWD in your .Renviron file.")
}
# Define core file paths based on project root
project_root <- here::here()  # Automatically finds root of project (e.g., RStudio project or Git repo)
download_dir <- file.path(project_root, "01_data", "parquet_clean")  # Folder to save downloaded .parquet files
duckdb_file <- file.path(project_root, "01_data", "buggdata_db.duckdb")  # Path to DuckDB database file
# NOTE: site_info.csv is hosted remotely
site_info_url <- "https://tabmon.nina.no/data/site_info.csv"  # Remote CSV with deployments metadata
site_info_file <- file.path(project_root, "01_data", "site_info.csv")  # Local dir to store it after download
# Create the download directory if it doesn't already exist
dir.create(download_dir, showWarnings = FALSE, recursive = TRUE)
source("02_scripts/01_config.R")
source("02_scripts/01_config.R")
source("02_scripts/01_config.R")
source("02_scripts/01_config.R")
# Load credentials from R environment
usr <- Sys.getenv("TABMON_USR")
psswd <- Sys.getenv("TABMON_PSSWD")
source("02_scripts/01_config.R")
# Load credentials from R environment
usr <- Sys.getenv("TABMON_USR")
psswd <- Sys.getenv("TABMON_PSSWD")
# Stop with error if credentials are not set
if (usr == "" || psswd == "") {
stop("Missing authentication credentials! Please set TABMON_USR and TABMON_PSSWD in your .Renviron file.")
}
# Load credentials from R environment
usr <- Sys.getenv("TABMON_USR")
psswd <- Sys.getenv("TABMON_PSSWD")
# Load credentials from R environment
usr <- Sys.getenv("TABMON_USR")
psswd <- Sys.getenv("TABMON_PSSWD")
# Stop with error if credentials are not set
if (usr == "" || psswd == "") {
stop("Missing authentication credentials! Please set TABMON_USR and TABMON_PSSWD in your .Renviron file.")
}
source("02_scripts/01_config.R")
source("02_scripts/01_config.R")
source("02_scripts/01_config.R")
library(arrow)
library(glue)
library(here)
library(httr)
library(janitor)
library(purrr)
library(readr)
library(tidyverse)
source("02_scripts/01_config.R")
# Download site_info.csv from the server if not already downloaded
if (!file.exists(site_info_file)) {
res <- GET(site_info_url, authenticate(usr, psswd), write_disk(site_info_file, overwrite = TRUE))
if (status_code(res) != 200) stop("Failed to download site_info.csv")
}
# Load and clean site_info (with janitor::make_clean_namesnames)
site_info <- read_csv(site_info_file, show_col_types = FALSE)
names(site_info) <- make_clean_names(names(site_info))
# Extract country and device ID from relevant columns
device_lookup <- site_info %>%
select(country, device_id) %>%
filter(!is.na(country), !is.na(device_id)) %>%
filter(country == "Netherlands") %>% ## To download only Netherlands data
distinct()
# Define months to consider
analysis_months <- c("2025-02", "2025-03", "2025-04", "2025-05", "2025-06") # add more here later if needed
# Generate all possible URLs
candidate_urls <- crossing(month = analysis_months, device_lookup) %>%
mutate(url = glue("https://tabmon.nina.no/data/parquet_results/country={country}/device_id={device_id}/{month}_{device_id}.parquet"))
# Function to check which URLs actually exist
check_exists <- function(url) {
res <- HEAD(url, authenticate(usr, psswd))
res$status_code == 200
}
# Run function
candidate_urls$exists <- map_lgl(candidate_urls$url, possibly(check_exists, otherwise = FALSE))
valid_files <- candidate_urls %>% filter(exists)
# Add local paths
valid_files <- valid_files %>%
mutate(local_path = file.path(download_dir, basename(url)))
walk2(valid_files$url, valid_files$local_path, ~ {
# Temporary location for download
temp_file <- tempfile(fileext = ".parquet")
# Download with credentials
res <- GET(.x, authenticate(usr, psswd), write_disk(temp_file, overwrite = TRUE))
if (status_code(res) != 200) {
warning(paste("Download failed:", .x))
return()
}
# Read file
df <- read_parquet(temp_file) %>% as_tibble()
# Normalize column names (replace spaces with underscores)
names(df) <- gsub(" ", "_", names(df))
# Extract device ID from the local path (captures what is before .parquet)
device_id <- str_match(.y, "([0-9a-f]{7,8})\\.parquet$")[, 2]
# Create time stamp + clip id + final filename
df <- df %>%
mutate(
common_name = str_to_lower(common_name),
scientific_name = str_to_lower(scientific_name),
device_id = device_id,
chunk_index = as.integer(start_time) / 3, # audio clip id in audio filenames (=start time/3)
filename_cleaned = str_remove(filename, "\\.mp3$") %>% # remove .mp3
paste(device_id, chunk_index, sep = "_"), # add device id and index in filename
recording_start_time = str_extract(filename_cleaned, "^[0-9T_\\.:-]+Z") %>% # extract time from filename
as.POSIXct(format = "%Y-%m-%dT%H_%M_%OSZ", tz = "UTC"), # format it properly
detection_time_utc = recording_start_time + as.numeric(start_time), # datetime column
)
# Final columns to keep
keep_cols <- c("filename_cleaned", "device_id", "detection_time_utc",
"scientific_name", "common_name", "confidence")
df_clean <- df %>%
select(any_of(keep_cols)) %>%
relocate(filename_cleaned) %>%  # optional: make filename first column
arrange(filename_cleaned) # orders by filename
# Save cleaned file
write_parquet(df_clean, .y)
})
## To adapt some manually downloaded files (sent by Corentin)
library(fs)
manual_files_dir <- "C:/Users/cbarile/Downloads"
manual_files <- dir_ls(manual_files_dir, regexp = "\\.parquet$")
# Clean and write them
walk(manual_files, function(file_path) {
# Read and normalize column names
df <- read_parquet(file_path) %>%
as_tibble()
names(df) <- gsub(" ", "_", names(df))
# Extract device ID from filename
device_id <- str_match(path_file(file_path), "([0-9a-f]{7,8})\\.parquet$")[, 2]
# Construct chunk index and cleaned filename
df <- df %>%
mutate(
common_name = str_to_lower(common_name),
scientific_name = str_to_lower(scientific_name),
device_id = device_id,
chunk_index = as.integer(start_time) / 3, # audio clip id in audio filenames (=start time/3)
filename_cleaned = str_remove(filename, "\\.mp3$") %>% # remove .mp3
paste(device_id, chunk_index, sep = "_"), # add device id and index in filename
recording_start_time = str_extract(filename_cleaned, "^[0-9T_\\.:-]+Z") %>% # extract time from filename
as.POSIXct(format = "%Y-%m-%dT%H_%M_%OSZ", tz = "UTC"), # format it properly
detection_time_utc = recording_start_time + as.numeric(start_time), # datetime column
)
# Final columns to keep
keep_cols <- c("filename_cleaned", "device_id", "detection_time_utc",
"scientific_name", "common_name", "confidence")
df_clean <- df %>%
select(any_of(keep_cols)) %>%
relocate(filename_cleaned) %>%
arrange(filename_cleaned)
# Output path (use same name)
out_file <- path(manual_files_dir, path_file(file_path))
write_parquet(df_clean, out_file)
})
manual_files_dir
manual_files <- dir_ls(manual_files_dir, regexp = "\\.parquet$")
manual_files
# Clean and write them
walk(manual_files, function(file_path) {
# Read and normalize column names
df <- read_parquet(file_path) %>%
as_tibble()
names(df) <- gsub(" ", "_", names(df))
# Extract device ID from filename
device_id <- str_match(path_file(file_path), "([0-9a-f]{7,8})\\.parquet$")[, 2]
# Construct chunk index and cleaned filename
df <- df %>%
mutate(
common_name = str_to_lower(common_name),
scientific_name = str_to_lower(scientific_name),
device_id = device_id,
chunk_index = as.integer(start_time) / 3, # audio clip id in audio filenames (=start time/3)
filename_cleaned = str_remove(filename, "\\.mp3$") %>% # remove .mp3
paste(device_id, chunk_index, sep = "_"), # add device id and index in filename
recording_start_time = str_extract(filename_cleaned, "^[0-9T_\\.:-]+Z") %>% # extract time from filename
as.POSIXct(format = "%Y-%m-%dT%H_%M_%OSZ", tz = "UTC"), # format it properly
detection_time_utc = recording_start_time + as.numeric(start_time), # datetime column
)
# Final columns to keep
keep_cols <- c("filename_cleaned", "device_id", "detection_time_utc",
"scientific_name", "common_name", "confidence")
df_clean <- df %>%
select(any_of(keep_cols)) %>%
relocate(filename_cleaned) %>%
arrange(filename_cleaned)
# Output path (use same name)
out_file <- path(manual_files_dir, path_file(file_path))
write_parquet(df_clean, out_file)
})
manual_files_dir <- "C:/Users/cbarile/Desktop"
manual_files <- dir_ls(manual_files_dir, regexp = "\\.parquet$")
manual_files
# Clean and write them
walk(manual_files, function(file_path) {
# Read and normalize column names
df <- read_parquet(file_path) %>%
as_tibble()
names(df) <- gsub(" ", "_", names(df))
# Extract device ID from filename
device_id <- str_match(path_file(file_path), "([0-9a-f]{7,8})\\.parquet$")[, 2]
# Construct chunk index and cleaned filename
df <- df %>%
mutate(
common_name = str_to_lower(common_name),
scientific_name = str_to_lower(scientific_name),
device_id = device_id,
chunk_index = as.integer(start_time) / 3, # audio clip id in audio filenames (=start time/3)
filename_cleaned = str_remove(filename, "\\.mp3$") %>% # remove .mp3
paste(device_id, chunk_index, sep = "_"), # add device id and index in filename
recording_start_time = str_extract(filename_cleaned, "^[0-9T_\\.:-]+Z") %>% # extract time from filename
as.POSIXct(format = "%Y-%m-%dT%H_%M_%OSZ", tz = "UTC"), # format it properly
detection_time_utc = recording_start_time + as.numeric(start_time), # datetime column
)
# Final columns to keep
keep_cols <- c("filename_cleaned", "device_id", "detection_time_utc",
"scientific_name", "common_name", "confidence")
df_clean <- df %>%
select(any_of(keep_cols)) %>%
relocate(filename_cleaned) %>%
arrange(filename_cleaned)
# Output path (use same name)
out_file <- path(manual_files_dir, path_file(file_path))
write_parquet(df_clean, out_file)
})
q
library(arrow)
library(glue)
library(here)
library(httr)
library(janitor)
library(purrr)
library(readr)
library(tidyverse)
source("02_scripts/01_config.R")
## To adapt some manually downloaded files (sent by Corentin)
library(fs)
manual_files_dir <- "C:/Users/cbarile/Desktop"
manual_files <- dir_ls(manual_files_dir, regexp = "\\.parquet$")
# Clean and write them
walk(manual_files, function(file_path) {
# Read and normalize column names
df <- read_parquet(file_path) %>%
as_tibble()
names(df) <- gsub(" ", "_", names(df))
# Extract device ID from filename
device_id <- str_match(path_file(file_path), "([0-9a-f]{7,8})\\.parquet$")[, 2]
# Construct chunk index and cleaned filename
df <- df %>%
mutate(
common_name = str_to_lower(common_name),
scientific_name = str_to_lower(scientific_name),
device_id = device_id,
chunk_index = as.integer(start_time) / 3, # audio clip id in audio filenames (=start time/3)
filename_cleaned = str_remove(filename, "\\.mp3$") %>% # remove .mp3
paste(device_id, chunk_index, sep = "_"), # add device id and index in filename
recording_start_time = str_extract(filename_cleaned, "^[0-9T_\\.:-]+Z") %>% # extract time from filename
as.POSIXct(format = "%Y-%m-%dT%H_%M_%OSZ", tz = "UTC"), # format it properly
detection_time_utc = recording_start_time + as.numeric(start_time), # datetime column
)
# Final columns to keep
keep_cols <- c("filename_cleaned", "device_id", "detection_time_utc",
"scientific_name", "common_name", "confidence")
df_clean <- df %>%
select(any_of(keep_cols)) %>%
relocate(filename_cleaned) %>%
arrange(filename_cleaned)
# Output path (use same name)
out_file <- path(manual_files_dir, path_file(file_path))
write_parquet(df_clean, out_file)
})
library(DBI)
library(dplyr)
library(duckdb)
library(janitor)
library(readr)
source("02_scripts/01_config.R")
# Connect to DuckDB (creates or opens existing DB)
con <- dbConnect(duckdb(duckdb_file))
# List all cleaned .parquet files from the .parquet data directory
cleaned_files <- list.files(download_dir, pattern = "\\.parquet$", full.names = TRUE)
# Build SQL query that unions all .parquet files into a virtual table
parquet_union <- paste0(
"SELECT * FROM read_parquet('", cleaned_files, "')",
collapse = " UNION ALL ")
# Create or replace DuckDB View called 'all_data' based on the unioned parquet files
dbExecute(con, paste0("CREATE OR REPLACE VIEW all_data AS ", parquet_union)) # should return 0
# Optional sanity check: see top rows
dbGetQuery(con, "SELECT * FROM all_data LIMIT 10")
# Ensure site_info.csv exists
if (!file.exists(site_info_file)) {
stop("site_info.csv not found. Run 02_download_data.R first.")
}
# Load the metadata file (site_info.csv)
site_info <- read_csv(site_info_file, show_col_types = FALSE)
# Extract only needed metadata columns and rename them for clarity
site_metadata <- site_info %>%
select(country, device_id, cluster, site, habitat = x12_habitat) %>%
distinct() %>%
mutate(across(where(is.character), tolower))
# Load the metadata file (site_info.csv)
site_info <- read_csv(site_info_file, show_col_types = FALSE)
# Clean column names (e.g., "X1 Country" becomes "x1_country")
names(site_info) <- make_clean_names(names(site_info))
# Extract only needed metadata columns and rename them for clarity
site_metadata <- site_info %>%
select(country, device_id, cluster, site, habitat = x12_habitat) %>%
distinct() %>%
mutate(across(where(is.character), tolower))
site_metadata
# Write metadata as DuckDB table so it can then be joined via SQL
dbWriteTable(con, "site_metadata", site_metadata, overwrite = TRUE)
dbExecute(con, "
CREATE OR REPLACE VIEW all_data_with_metadata AS
SELECT
a.*,                                  -- all data columns (filename, timestamp, etc.)
m.country,                            -- join: add country from site_metadata
m.cluster,                            -- join: add cluster from site_metadata
m.site,                               -- join: add site from site_metadata
m.habitat                             -- join: add habitat from site_metadata
FROM all_data a                         -- alias `a` = all_data view
LEFT JOIN site_metadata m               -- alias `m` = site_metadata table
ON a.device_id = m.device_id          -- join condition: matching device_ID
")
# Optional: sanity check on joined data
df_preview <- dbGetQuery(con, "SELECT * FROM all_data_with_metadata LIMIT 100")
View(df_preview)
# Disconnect from database
dbDisconnect(con)
library(DBI)
library(dplyr)
library(duckdb)
library(janitor)
library(readr)
source("02_scripts/01_config.R")
# Connect to DuckDB (creates or opens existing DB)
db_connect <- dbConnect(duckdb(duckdb_file))
# List all cleaned .parquet files from the .parquet data directory
cleaned_files <- list.files(download_dir, pattern = "\\.parquet$", full.names = TRUE)
# Build SQL query that unions all .parquet files into a virtual table
parquet_union <- paste0(
"SELECT * FROM read_parquet('", cleaned_files, "')",
collapse = " UNION ALL ")
# Create or replace DuckDB View called 'all_data' based on the unioned parquet files
dbExecute(db_connect, paste0("CREATE OR REPLACE VIEW all_data AS ", parquet_union)) # should return 0
# Optional sanity check: see top rows
dbGetQuery(db_connect, "SELECT * FROM all_data LIMIT 10")
# Ensure site_info.csv exists
if (!file.exists(site_info_file)) {
stop("site_info.csv not found. Run 02_download_data.R first.")
}
# Load the metadata file (site_info.csv)
site_info <- read_csv(site_info_file, show_col_types = FALSE)
# Clean column names (e.g., "X1 Country" becomes "x1_country")
names(site_info) <- make_clean_names(names(site_info))
names(site_info)
# Extract only needed metadata columns and rename them for clarity
site_metadata <- site_info %>%
select(country, device_id, cluster, site, habitat = x12_habitat, latitude, longitude) %>%
distinct() %>%
mutate(across(where(is.character), tolower))
# Write metadata as DuckDB table so it can then be joined via SQL
dbWriteTable(db_connect, "site_metadata", site_metadata, overwrite = TRUE)
dbExecute(db_connect, "
CREATE OR REPLACE VIEW all_data_with_metadata AS
SELECT
a.*,                                  -- all data columns (filename, timestamp, etc.)
m.country,                            -- join: add country from site_metadata
m.cluster,                            -- join: add cluster from site_metadata
m.site,                               -- join: add site from site_metadata
m.habitat                             -- join: add habitat from site_metadata
m.latitude                            -- join: add latitude from site_metadata
m.longitude                           -- join: add longitude from site_metadata
FROM all_data a                         -- alias `a` = all_data view
LEFT JOIN site_metadata m               -- alias `m` = site_metadata table
ON a.device_id = m.device_id          -- join condition: matching device_ID
")
dbExecute(db_connect, "
CREATE OR REPLACE VIEW all_data_with_metadata AS
SELECT
a.*,                                  -- all data columns (filename, timestamp, etc.)
m.country,                            -- join: add country from site_metadata
m.cluster,                            -- join: add cluster from site_metadata
m.site,                               -- join: add site from site_metadata
m.habitat,                            -- join: add habitat from site_metadata
m.latitude,                           -- join: add latitude from site_metadata
m.longitude                           -- join: add longitude from site_metadata
FROM all_data a                         -- alias `a` = all_data view
LEFT JOIN site_metadata m               -- alias `m` = site_metadata table
ON a.device_id = m.device_id          -- join condition: matching device_ID
")
# Optional: sanity check on joined data
df_preview <- dbGetQuery(db_connect, "SELECT * FROM all_data_with_metadata LIMIT 100")
View(df_preview)
# Disconnect from database
dbDisconnect(db_connect)
